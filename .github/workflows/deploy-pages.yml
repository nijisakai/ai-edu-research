name: Deploy Dashboard to GitHub Pages

on:
  push:
    branches: [main]
    paths:
      - 'docs/**'
      - 'output/*.json'
      - 'output/*.md'
      - 'output/figures/**'
      - 'output/paper/**'
      - 'output/report/**'
      - '*.md'
      - '.github/workflows/deploy-pages.yml'
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Sync JSON data to docs/data/
        run: |
          mkdir -p docs/data
          for f in output/*.json; do
            [ -f "$f" ] && cp "$f" docs/data/
          done

      - name: Copy figures to docs/figures/
        run: |
          mkdir -p docs/figures
          cp output/figures/*.png docs/figures/ 2>/dev/null || true

      - name: Copy paper and report HTML
        run: |
          mkdir -p docs/paper docs/report
          cp "output/paper/论文_AI赋能基础教育的实践图景与产业生态.html" docs/paper/content.html 2>/dev/null || true
          cp "output/report/研究报告_AI赋能基础教育实践图景与产业生态分析.html" docs/report/content.html 2>/dev/null || true

      - name: Fix image paths in paper/report
        run: |
          for f in docs/paper/content.html docs/report/content.html; do
            if [ -f "$f" ]; then
              python3 -c "
          import sys
          with open('$f', 'r', encoding='utf-8') as fh:
              content = fh.read()
          old = 'file:///Users/sakai/Desktop/产业调研/ai-edu-research/output/figures/'
          count = content.count(old)
          content = content.replace(old, '../figures/')
          with open('$f', 'w', encoding='utf-8') as fh:
              fh.write(content)
          print(f'Fixed {count} image paths in $f')
              "
            fi
          done

      - name: Convert MD documents to HTML
        run: |
          pip install markdown > /dev/null 2>&1
          mkdir -p docs/docs
          python3 -c "
          import markdown, os

          TEMPLATE = '''<!DOCTYPE html>
          <html lang=\"{lang}\">
          <head>
          <meta charset=\"UTF-8\">
          <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">
          <title>{title} — AI赋能基础教育研究</title>
          <meta name=\"author\" content=\"陈虹宇\">
          <link rel=\"stylesheet\" href=\"../css/style.css\">
          </head>
          <body>
          <script src=\"../js/nav.js\"></script>
          <div class=\"page-container doc-content\">{content}</div>
          <footer class=\"site-footer\">
            <p>AI赋能基础教育的实践图景与产业生态 — 基于1690个典型案例的混合方法研究</p>
            <p>研究者：陈虹宇 | <a href=\"https://github.com/nijisakai/ai-edu-research\" target=\"_blank\">GitHub</a></p>
          </footer>
          </body>
          </html>'''

          files = [
              ('output/案例深度分析摘要.md', 'docs/docs/case-analysis-zh.html', '案例深度分析摘要', 'zh-CN'),
              ('output/因果与统计分析报告.md', 'docs/docs/causal-analysis-zh.html', '因果与统计分析报告', 'zh-CN'),
              ('output/深度洞察挖掘报告.md', 'docs/docs/deep-insights-zh.html', '深度洞察挖掘报告', 'zh-CN'),
              ('output/AI基础教育产业研究简报.md', 'docs/docs/industry-research-zh.html', '产业研究简报', 'zh-CN'),
              ('output/case_deep_analysis_summary.md', 'docs/docs/case-analysis-en.html', 'Case Analysis Summary', 'en'),
              ('output/causal_analysis_summary.md', 'docs/docs/causal-analysis-en.html', 'Causal Analysis Summary', 'en'),
              ('output/deep_insights_summary.md', 'docs/docs/deep-insights-en.html', 'Deep Insights Summary', 'en'),
              ('output/industry_research.md', 'docs/docs/industry-research-en.html', 'Industry Research', 'en'),
              ('output/huang_ronghuai_research.md', 'docs/docs/huang-ronghuai.html', '黄荣怀理论研究', 'zh-CN'),
              ('论文审读报告.md', 'docs/docs/paper-review.html', '论文审读报告', 'zh-CN'),
              ('报告审读报告.md', 'docs/docs/report-review.html', '报告审读报告', 'zh-CN'),
              ('技术说明与图表解读指南.md', 'docs/docs/tech-guide.html', '技术说明与图表解读指南', 'zh-CN'),
          ]

          extensions = ['tables', 'fenced_code', 'toc']

          for src, dst, title, lang in files:
              if not os.path.exists(src):
                  print(f'SKIP {src}')
                  continue
              with open(src, 'r', encoding='utf-8') as f:
                  md_text = f.read()
              html_content = markdown.markdown(md_text, extensions=extensions)
              page = TEMPLATE.format(lang=lang, title=title, content=html_content)
              with open(dst, 'w', encoding='utf-8') as f:
                  f.write(page)
              print(f'OK {src} -> {dst}')
          "

      - name: Anonymize data files
        run: |
          python3 -c "
          import json, os

          # 1. Remove _case_texts.json (contains PII)
          texts_path = 'docs/data/_case_texts.json'
          if os.path.exists(texts_path):
              os.remove(texts_path)
              print('Deleted _case_texts.json')

          # 2. Anonymize case_deep_analysis.json (school names)
          path = 'docs/data/case_deep_analysis.json'
          if os.path.exists(path):
              with open(path, 'r', encoding='utf-8') as f:
                  data = json.load(f)
              schools = sorted(set(c.get('school','') for c in data.get('cases',[])))
              schools = [s for s in schools if s and s != '未明确']
              school_map = {s: f'学校{chr(65+i)}' for i, s in enumerate(schools)}
              school_map['未明确'] = '未明确'
              for case in data.get('cases', []):
                  if case.get('school') in school_map:
                      case['school'] = school_map[case['school']]
              with open(path, 'w', encoding='utf-8') as f:
                  json.dump(data, f, ensure_ascii=False, indent=2)
              print(f'Anonymized {len(school_map)} schools')

          # 3. Anonymize _case_selection.json (teacher names, filepaths)
          path = 'docs/data/_case_selection.json'
          if os.path.exists(path):
              with open(path, 'r', encoding='utf-8') as f:
                  sel = json.load(f)
              for key, case in sel.items():
                  case.pop('filepath', None)
                  title = case.get('title', '')
                  case['filename'] = f'第{key}号案例_{title}'
              with open(path, 'w', encoding='utf-8') as f:
                  json.dump(sel, f, ensure_ascii=False, indent=2)
              print(f'Anonymized {len(sel)} case selection entries')
          "

      - name: Generate API index
        run: |
          python3 -c "
          import json, os, glob
          from datetime import date
          files = sorted(glob.glob('docs/data/*.json'))
          index = {
            'version': '1.0',
            'project': 'AI赋能基础教育研究',
            'author': '陈虹宇',
            'description': '1,690个典型案例的分析数据API（密钥：BNUSLI）',
            'base_url': 'https://nijisakai.github.io/ai-edu-research/data/',
            'updated': str(date.today()),
            'files': {}
          }
          for f in files:
            name = os.path.basename(f)
            if name == 'api-index.json':
              continue
            size = os.path.getsize(f)
            index['files'][name] = {
              'path': 'data/' + name,
              'size_kb': round(size / 1024, 1)
            }
          with open('docs/data/api-index.json', 'w', encoding='utf-8') as out:
            json.dump(index, out, ensure_ascii=False, indent=2)
          "

      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
